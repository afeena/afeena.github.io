<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>memory on Evgeniia Tokarchuk</title>
    <link>https://evgeniia.tokarch.uk/tags/memory/</link>
    <description>Recent content in memory on Evgeniia Tokarchuk</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 30 Mar 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://evgeniia.tokarch.uk/tags/memory/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>One example of memory optimization for KL-loss calculation in pytorch</title>
      <link>https://evgeniia.tokarch.uk/blog/memory-optimization-for-kl-loss-calculation-in-pytorch/</link>
      <pubDate>Thu, 30 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>https://evgeniia.tokarch.uk/blog/memory-optimization-for-kl-loss-calculation-in-pytorch/</guid>
      <description>Memory usage is a common issue for large ML models. Especially in academia, we have to use resources wisely and make the most out of resources available. While working on my mixture modelâ€™s KL-objective, I have to make some less common optimization to reduce memory usage.
Setup Decoder outputs a large matrix (O) with dimensionality ((M \times B \times L \times D)) where (M) is the number of clusters, (B) is a batch size, (L) is a sequence lengths and (D) is model output dimension.</description>
    </item>
    
  </channel>
</rss>
