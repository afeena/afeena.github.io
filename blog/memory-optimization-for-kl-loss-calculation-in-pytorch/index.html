<!DOCTYPE html>

<html lang="en-us"><head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css"
        integrity="sha384-TX8t27EcRE3e/ihU7zmQxVncDAy5uIKz4rEkgIXeMed4M0jlfIDPvg6uqKI2xXr2" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Nanum+Myeongjo&family=Noto+Serif+JP&family=Cormorant+Garamond&family=Libre+Baskerville&family=Source+Serif+Pro&family=Crimson+Text&family=Inter&family=Crimson+Pro&family=Literata&family=Ubuntu+Mono&family=Inter&family=Roboto">
    <link rel="stylesheet" type="text/css" href="/css/style.css">

    
    

    <title>Evgeniia Tokarchuk | One example of memory optimization for KL-loss calculation in pytorch</title>


    

</head><body class="container d-flex flex-column min-vh-100">

<div class="blog_nav_bar secondary_font ">
    
    
    <a class="navbar-brand" href="/">about</a>
    
    
    
    <a class="navbar-brand" href="/blog">« all posts</a>
    
    
</div>



<h3>
    <a class="title" href="/blog/memory-optimization-for-kl-loss-calculation-in-pytorch/">One example of memory optimization for KL-loss calculation in pytorch</a>
</h3>

<div class="reading_time secondary_font text-muted ">
    <span>
        Mar 30 2023 · 4 min read
    </span>

</div>




<div class="tags_navigation">
    
    <a class="tag" href="/tags/pytorch/">#pytorch</a>
    
    <a class="tag" href="/tags/memory/">#memory</a>
    
    <a class="tag" href="/tags/fairseq/">#fairseq</a>
    
    <a class="tag" href="/tags/nlp/">#nlp</a>
    
    <a class="tag" href="/tags/machine-learning/">#machine-learning</a>
    
</div>


<p>Memory usage is a common issue for large ML models. Especially in academia, we have to use resources wisely and make the most out of resources available. While working on my mixture model’s KL-objective, I have to make some less common optimization to reduce memory usage.</p>
<h2 id="setup">Setup</h2>
<p>Decoder outputs a large matrix $$O$$ with dimensionality $$(M \times B \times L \times D)$$ where $$M$$ is the number of clusters, $$B$$ is a batch size, $$L$$ is a sequence lengths and $$D$$ is model output dimension. Lets assume from now on that $$T = (L \times D)$$ so we an reduce dim by 1.</p>
<ol>
<li>Each raw of matrix $$O$$ represents mixture component $$Q_m = PowerSpherical(\mu_m, \kappa_m)$$ where each row $$m$$ is a location value $$\mu_m$$ and a spread parameter $$\kappa_m = ||\mu_m||$$ and $$Q(x) = \sum_{1}^{M} p(m) Q_m(x)$$</li>
<li>We sample $$z_m$$ from each mixture component $$Q_m$$ and get a matrix of samples, where each raw is a sample of component $$m$$.</li>
<li>Now, our goal is to calculate $$\log q_j(z_m)$$, i.e., for each sample $$z_m$$ calculate log probabilities given all mixture components $$q_j$$ where $$j\in[1,M]$$  So we end up with the matrix of $$M \times M \times T$$</li>
</ol>
<p>Now, let’s talk code. We use implementation of PowerSpherical distribution provided by Nicola De Cao (<a href="https://github.com/nicola-decao/power_spherical">https://github.com/nicola-decao/power_spherical</a>) [1]. In particular we are interested in one function <code>log_prob</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e">#[1] De Cao, N., Aziz, W. (2020). </span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#The Power Spherical distrbution.</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#In Proceedings of the 37th International </span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#Conference on Machine Learning, INNF+.</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#https://github.com/nicola-decao/power_spherical/blob/master/power_spherical/distributions.py#L176-L189</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">log_prob</span>(self, value):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>log_normalizer() <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>scale <span style="color:#f92672">*</span> torch<span style="color:#f92672">.</span>log1p(
</span></span><span style="display:flex;"><span>            (self<span style="color:#f92672">.</span>loc <span style="color:#f92672">*</span> value)<span style="color:#f92672">.</span>sum(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">log_normalizer</span>(self):
</span></span><span style="display:flex;"><span>        alpha <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>base_dist<span style="color:#f92672">.</span>marginal_t<span style="color:#f92672">.</span>base_dist<span style="color:#f92672">.</span>concentration1
</span></span><span style="display:flex;"><span>        beta <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>base_dist<span style="color:#f92672">.</span>marginal_t<span style="color:#f92672">.</span>base_dist<span style="color:#f92672">.</span>concentration0
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> <span style="color:#f92672">-</span>(
</span></span><span style="display:flex;"><span>            (alpha <span style="color:#f92672">+</span> beta) <span style="color:#f92672">*</span> math<span style="color:#f92672">.</span>log(<span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">+</span> torch<span style="color:#f92672">.</span>lgamma(alpha)
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">-</span> torch<span style="color:#f92672">.</span>lgamma(alpha <span style="color:#f92672">+</span> beta)
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">+</span> beta <span style="color:#f92672">*</span> math<span style="color:#f92672">.</span>log(math<span style="color:#f92672">.</span>pi)
</span></span><span style="display:flex;"><span>        )
</span></span></code></pre></div><p>Let’s look closely at one line</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>(self<span style="color:#f92672">.</span>loc <span style="color:#f92672">*</span> value)<span style="color:#f92672">.</span>sum(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
</span></span></code></pre></div><p>To get probability of each mixture for each value, we need to involve <code>broadcasting</code>, so we expand sample <code>z_m</code> at dimension 1, <code>value</code> is $$M \times 1 \times T \times D$$ and <code>loc</code> is $$M \times T \times D$$</p>
<p>Then <code>self.loc * value</code> would be a $$M \times M \times T \times D$$ matrix, and then we do <code>sum</code> over last dimension. Remember, in NLP we typically work with high-dimensional vectors. Scary.</p>
<h2 id="dive-into-optimization">Dive into optimization</h2>
<p>Lets assume our typical numbers for M,T and D. M=128, T=512, D=128 and see how much memory it takes to get log probs</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> math
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.autograd.profiler <span style="color:#66d9ef">as</span> profiler
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> power_spherical
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn.functional <span style="color:#66d9ef">as</span> F
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">call_log_prob</span>(q, z_smaples):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> q<span style="color:#f92672">.</span>log_prob(z_smaples<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> __name__<span style="color:#f92672">==</span><span style="color:#e6db74">&#34;__main__&#34;</span>:
</span></span><span style="display:flex;"><span>    out <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>rand(<span style="color:#ae81ff">128</span>,<span style="color:#ae81ff">512</span>,<span style="color:#ae81ff">128</span>)
</span></span><span style="display:flex;"><span>    scale <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>norm(out, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    loc <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>normalize(out, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    q <span style="color:#f92672">=</span> power_spherical<span style="color:#f92672">.</span>PowerSpherical(loc, scale)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    z_samples <span style="color:#f92672">=</span> q<span style="color:#f92672">.</span>rsample()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> profiler<span style="color:#f92672">.</span>profile(
</span></span><span style="display:flex;"><span>        use_cuda<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, with_stack<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, profile_memory<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>
</span></span><span style="display:flex;"><span>        ) <span style="color:#66d9ef">as</span> prof:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">with</span> profiler<span style="color:#f92672">.</span>record_function(<span style="color:#e6db74">&#34;call_log_prob&#34;</span>):
</span></span><span style="display:flex;"><span>            val1 <span style="color:#f92672">=</span> call_log_prob(q, z_samples)
</span></span><span style="display:flex;"><span>    print(prof<span style="color:#f92672">.</span>key_averages(group_by_stack_n<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>)<span style="color:#f92672">.</span>table(sort_by<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;self_cuda_time_total&#34;</span>, row_limit<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>))
</span></span></code></pre></div><p>This operation requires <strong>4.1GB</strong> of cuda memory! And we cannot run it with more than T=512!</p>
<p>As we mentioned above, <code>(self.loc*value).sum(-1)</code> requires multiplication with broadcasting and then summation. Very convenient, we can use <code>einsum</code> [3,4] instead!</p>
<p>However, since we call external function of a distribution, we cannot use <code>TorchScript</code> [2] on this function! Let’s calculate <code>log_normalizer</code> on the fly.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> math
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.autograd.profiler <span style="color:#66d9ef">as</span> profiler
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> power_spherical
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn.functional <span style="color:#66d9ef">as</span> F
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">call_log_prob</span>(q, z_smaples):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> q<span style="color:#f92672">.</span>log_prob(z_smaples<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">calc_log_prob_partial</span>(q, value):
</span></span><span style="display:flex;"><span>    log_prob <span style="color:#f92672">=</span> q<span style="color:#f92672">.</span>log_normalizer()<span style="color:#f92672">+</span>q<span style="color:#f92672">.</span>scale <span style="color:#f92672">*</span> torch<span style="color:#f92672">.</span>log1p(
</span></span><span style="display:flex;"><span>                torch<span style="color:#f92672">.</span>einsum(<span style="color:#e6db74">&#34;ijk, ljk -&gt; lij&#34;</span>, q<span style="color:#f92672">.</span>loc, value)
</span></span><span style="display:flex;"><span>            )
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> log_prob
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">@torch</span><span style="color:#f92672">.</span>jit<span style="color:#f92672">.</span>script
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">calc_log_prob</span>(loc, scale, value):
</span></span><span style="display:flex;"><span>    beta <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor([(loc<span style="color:#f92672">.</span>shape[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>] <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">/</span> <span style="color:#ae81ff">2</span>], device<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;cuda&#39;</span>)
</span></span><span style="display:flex;"><span>    alpha <span style="color:#f92672">=</span> beta <span style="color:#f92672">+</span> scale
</span></span><span style="display:flex;"><span>    log_prob <span style="color:#f92672">=</span> <span style="color:#f92672">-</span>(
</span></span><span style="display:flex;"><span>                    (alpha <span style="color:#f92672">+</span> beta) <span style="color:#f92672">*</span> math<span style="color:#f92672">.</span>log(<span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>                    <span style="color:#f92672">+</span> torch<span style="color:#f92672">.</span>lgamma(alpha)
</span></span><span style="display:flex;"><span>                    <span style="color:#f92672">-</span> torch<span style="color:#f92672">.</span>lgamma(alpha <span style="color:#f92672">+</span> beta)
</span></span><span style="display:flex;"><span>                    <span style="color:#f92672">+</span> beta <span style="color:#f92672">*</span> math<span style="color:#f92672">.</span>log(math<span style="color:#f92672">.</span>pi)
</span></span><span style="display:flex;"><span>            ) <span style="color:#f92672">+</span> scale <span style="color:#f92672">*</span> torch<span style="color:#f92672">.</span>log1p(
</span></span><span style="display:flex;"><span>                torch<span style="color:#f92672">.</span>einsum(<span style="color:#e6db74">&#34;ijk, ljk -&gt; lij&#34;</span>, loc, value)
</span></span><span style="display:flex;"><span>            )
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> log_prob
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> __name__<span style="color:#f92672">==</span><span style="color:#e6db74">&#34;__main__&#34;</span>:
</span></span><span style="display:flex;"><span>    out <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>rand(<span style="color:#ae81ff">128</span>,<span style="color:#ae81ff">512</span>,<span style="color:#ae81ff">128</span>)
</span></span><span style="display:flex;"><span>    scale <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>norm(out, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    loc <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>normalize(out, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    q <span style="color:#f92672">=</span> power_spherical<span style="color:#f92672">.</span>PowerSpherical(loc, scale)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    z_samples <span style="color:#f92672">=</span> q<span style="color:#f92672">.</span>rsample()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> profiler<span style="color:#f92672">.</span>profile(
</span></span><span style="display:flex;"><span>        use_cuda<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, with_stack<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, profile_memory<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>
</span></span><span style="display:flex;"><span>        ) <span style="color:#66d9ef">as</span> prof:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">with</span> profiler<span style="color:#f92672">.</span>record_function(<span style="color:#e6db74">&#34;call_log_prob&#34;</span>):
</span></span><span style="display:flex;"><span>            val1 <span style="color:#f92672">=</span> call_log_prob(q, z_samples)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">with</span> profiler<span style="color:#f92672">.</span>record_function(<span style="color:#e6db74">&#34;calc_log_prob_partial&#34;</span>):
</span></span><span style="display:flex;"><span>            val2 <span style="color:#f92672">=</span> calc_log_prob_partial(q, z_samples)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">with</span> profiler<span style="color:#f92672">.</span>record_function(<span style="color:#e6db74">&#34;calc_log_prob&#34;</span>):
</span></span><span style="display:flex;"><span>            val3 <span style="color:#f92672">=</span> calc_log_prob(q<span style="color:#f92672">.</span>loc, q<span style="color:#f92672">.</span>scale, z_samples)
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">#ensure correctness of computations</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">assert</span> torch<span style="color:#f92672">.</span>allclose(val1, val2)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">assert</span> torch<span style="color:#f92672">.</span>allclose(val1, val3)
</span></span><span style="display:flex;"><span>    print(prof<span style="color:#f92672">.</span>key_averages(group_by_stack_n<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>)<span style="color:#f92672">.</span>table(sort_by<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;self_cuda_time_total&#34;</span>, row_limit<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>))
</span></span></code></pre></div><p>Lets combine everything in the table! We are mostly interested in Self CUDA Mem and CUDA time avg</p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Self CUDA Mem</th>
<th>CUDA time avg</th>
<th>CPU total %</th>
<th>CPU total</th>
<th>CPU time avg</th>
<th>Self CUDA</th>
<th>Self CUDA %</th>
</tr>
</thead>
<tbody>
<tr>
<td>call_log_prob</td>
<td>-4.10Gb</td>
<td>71.429ms</td>
<td>1.84</td>
<td>11.105ms</td>
<td>11.105ms</td>
<td>419.000us</td>
<td>0.07</td>
</tr>
<tr>
<td>calc_log_prob_partial</td>
<td>-98.50Mb</td>
<td>466.243ms</td>
<td>87.03</td>
<td>524.771ms</td>
<td>524.771ms</td>
<td>38.000us</td>
<td>0.01</td>
</tr>
<tr>
<td>calc_log_prob torchscript</td>
<td>0b</td>
<td>63.637ms</td>
<td>10.36</td>
<td>63.455ms</td>
<td>63.455ms</td>
<td>60.112ms</td>
<td>9.81</td>
</tr>
</tbody>
</table>
<p>Memory usage on GPU 4.1Gb vs 98.5 Mb vs 0 (?!) with <code>torch.jit.script</code>, ~40 times less memory usage (!!!!). Impressive, right? And with <code>torch.jit.script</code> we do not suffer from computational time increase.</p>
<h2 id="lessons-learned"><strong>Lessons learned:</strong></h2>
<ol>
<li>If you have huge matrix multiplication followed by sum → use <code>einsum</code></li>
<li>If you can use <code>TorchScript</code>, use it. You might need to get rid of some external functions and your code will be less readable. However it saves both memory and computational time.</li>
</ol>
<p>[1] De Cao, N., Aziz, W. (2020). The Power Spherical distrbution. In Proceedings of the 37th International Conference on Machine Learning, INNF+.</p>
<p>[2] <a href="https://pytorch.org/docs/stable/jit.html">https://pytorch.org/docs/stable/jit.html</a></p>
<p>[3] <a href="https://pytorch.org/docs/stable/generated/torch.einsum.html">https://pytorch.org/docs/stable/generated/torch.einsum.html</a></p>
<p>[4] <a href="https://rockt.github.io/2018/04/30/einsum">https://rockt.github.io/2018/04/30/einsum</a></p>


<footer class="mt-auto d-flex justify-content-center text-muted small secondary_font">
    <span class="text-muted">Copyright (c) 2023, E Tokarchuk,
        <a class="text-muted" href="https://github.com/hadisinaee/avicenna" target="_blank"> created by Avicenna
            (MIT)</a>
    </span>
</footer><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.bundle.min.js"
    integrity="sha384-ho+j7jyWK8fNQe+A12Hb8AhRq26LrZ/JpcUGGOn+Y7RsweNrtN/tE3MoK7ZeZDyx"
    crossorigin="anonymous"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/feather-icons/4.28.0/feather.min.js"></script>
<script>
    feather.replace()
</script></body>

</html>